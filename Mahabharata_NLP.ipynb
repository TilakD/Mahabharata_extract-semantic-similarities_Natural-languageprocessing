{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "**Word Vector from Mahabaratha**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from __future__ import absolute_import, division, print_function",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import codecs\nimport glob\nimport logging\nimport multiprocessing\nimport os\nimport pprint\nimport re",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import nltk\nimport gensim.models.word2vec as w2v\nimport sklearn.manifold\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "%pylab inline",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Set up logging",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Prepare Corpus\nLoad books from files",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "book_filenames = sorted(glob.glob(\"../input/*.txt\"))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"Found books:\")\nbook_filenames",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Combine the books into one string",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "corpus_raw = u\"\"\nfor book_filename in book_filenames:\n    print(\"Reading '{0}'...\".format(book_filename))\n    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n        corpus_raw += book_file.read()\n    print(\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n    print()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Split the corpus into sentences",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "raw_sentences = tokenizer.tokenize(corpus_raw)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#convert into a list of words\n#remove unnnecessary, split into words, no hyphens\n#list of words\ndef sentence_to_wordlist(raw):\n    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n    words = clean.split()\n    return words",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#sentence where each word is tokenized\nsentences = []\nfor raw_sentence in raw_sentences:\n    if len(raw_sentence) > 0:\n        sentences.append(sentence_to_wordlist(raw_sentence))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(raw_sentences[5])\nprint(sentence_to_wordlist(raw_sentences[5]))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "token_count = sum([len(sentence) for sentence in sentences])\nprint(\"The book corpus contains {0:,} tokens\".format(token_count))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Train Word2Vec",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#ONCE we have vectors\n#step 3 - build model\n#3 main tasks that vectors help with\n#DISTANCE, SIMILARITY, RANKING\n\n# Dimensionality of the resulting word vectors.\n#more dimensions, more computationally expensive to train\n#but also more accurate\n#more dimensions = more generalized\nnum_features = 300\n# Minimum word count threshold.\nmin_word_count = 3\n\n# Number of threads to run in parallel.\n#more workers, faster we train\nnum_workers = multiprocessing.cpu_count()\n\n# Context window length.\ncontext_size = 7\n\n# Downsample setting for frequent words.\n#0 - 1e-5 is good for this\ndownsampling = 1e-3\n\n# Seed for the RNG, to make the results reproducible.\n#random number generator\n#deterministic, good for debugging\nseed = 1",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mahabharata2vec = w2v.Word2Vec(\n    sg=1,\n    seed=seed,\n    workers=num_workers,\n    size=num_features,\n    min_count=min_word_count,\n    window=context_size,\n    sample=downsampling\n)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mahabharata2vec.build_vocab(sentences)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"Word2Vec vocabulary length:\", len(mahabharata2vec.wv.vocab))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Start training, this might take a minute or two...",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mahabharata2vec.train(sentences)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Save to file, can be useful later",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "if not os.path.exists(\"trained\"):\n    os.makedirs(\"trained\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mahabharata2vec.save(os.path.join(\"trained\", \"mahabharata2vec.w2v\"))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Explore the trained model.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mahabharata2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"mahabharata2vec.w2v\"))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Compress the word vectors into 2D space and plot them",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#my video - how to visualize a dataset easily\ntsne = sklearn.manifold.TSNE(n_components=3, random_state=0)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "all_word_vectors_matrix = mahabharata2vec.wv.syn0",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Train t-SNE, this could take a minute or two...",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Plot the big picture",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "points = pd.DataFrame(\n    [\n        (word, coords[0], coords[1], coords[2])\n        for word, coords in [\n            (word, all_word_vectors_matrix_2d[mahabharata2vec.wv.vocab[word].index])\n            for word in mahabharata2vec.wv.vocab\n        ]\n    ],\n    columns=[\"word\", \"x\", \"y\", \"z\"]\n)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "points.head(10)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.set_context(\"poster\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "points.plot.scatter(\"x\", \"y\", c = \"z\",s=10, figsize=(12, 12))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def plot_region(x_bounds, y_bounds):\n    slice = points[\n        (x_bounds[0] <= points.x) &\n        (points.x <= x_bounds[1]) & \n        (y_bounds[0] <= points.y) &\n        (points.y <= y_bounds[1])\n    ]\n    \n    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n    for i, point in slice.iterrows():\n        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "People related to Kingsguard ended up together",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plot_region(x_bounds=(4.0, 4.2), y_bounds=(-0.5, -0.1))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Food products are grouped nicely as well. Aerys (The Mad King) being close to \"roasted\" also looks sadly correct",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "plot_region(x_bounds=(0, 1), y_bounds=(4, 4.5))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Explore semantic similarities between book characters. Words closest to the given word",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mahabharata2vec.most_similar(\"Krishna\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mahabharata2vec.most_similar(\"Arjuna\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mahabharata2vec.most_similar(\"Karna\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "mahabharata2vec.most_similar(\"Vrishasena\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Linear relationships between word pairs",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def nearest_similarity_cosmul(start1, end1, end2):\n    similarities = mahabharata2vec.most_similar_cosmul(\n        positive=[end2, start1],\n        negative=[end1]\n    )\n    start2 = similarities[0][0]\n    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n    return start2",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "nearest_similarity_cosmul(\"Dhritarastra\", \"Pandu\", \"Nakula\")\nnearest_similarity_cosmul(\"Bhima\", \"Arjuna\", \"Ambika\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from nltk.tag import pos_tag\n\nsentence = \"Vrishasena Ambalika at the death of Duhshasana and Chitrasena rushed against Nakula desiring to fight with his father's enemy. A fierce battle then ensued between those two heroes. Vrishasena managed to kill Nakula's horses and pierce him with many arrows. Descending from his chariot, Nakula took up his sword and shield, and making his way toward Vrishasena, he severed the heads of two thousand horsemen. Vrishasena, seeing Nakula coming towards him whirling that sword like a discus, shattered the sword and shield with four crescent shaped arrows. Nakula then quickly ascended Bhima's chariot. As Arjuna came near, Nakula requested him Please slay this sinful person Arjuna then ordered Lord Krishna Proceed toward the son of Karna.\"\ntagged_sent = pos_tag(sentence.split())\nprint (tagged_sent)\n\npropernouns = [word for word,pos in tagged_sent if pos == 'NNP']\nprint (propernouns)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}