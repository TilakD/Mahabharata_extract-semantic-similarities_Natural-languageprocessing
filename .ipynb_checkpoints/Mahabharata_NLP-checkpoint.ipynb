{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Capstone Project\n",
    "\n",
    "### Project: Semantic similarity extraction using word vectors in Mahabharata dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the capstone project of the Machine Learning Engineer Nanodegree! In this notebook, we will use corpus of words from Mahabharata is used as an input to create word vectors using word2vec, with the help of t-SNE, reduce the dimensions of the word vectors and finally use cosine similarity to analyze semantic similarities, i.e. to answer relationship questions based on the learning. The end solution of this project will be to analyze relationships and logics in the dataset. \n",
    "\n",
    "The dataset for this project can be found on the [GitHub Mahabharata Machine Learning Repository](https://github.com/TilakD/Mahabharata_extract-semantic-similarities_Natural-languageprocessing/tree/master/Dataset)\n",
    "\n",
    "Model is assessed using the real facts about the data set, to benchmark the model I have compiled 23 relationship facts and will be adding few more as I build the model. For example, below are a few of the real data used to benchmark the model.\n",
    "Dhritarastra is related to Pandu, as Sahadeva is related to Nakula\n",
    "\n",
    "    Bhima is related to Arjuna, as Ambalika is related to Ambika\n",
    "    Pandu is related to Kunti, as Dhritarashtra is related to Gandhari\n",
    "    Bhima is related to Draupadi, as Arjuna is related to Chitrangada\n",
    "    Karna is related to Kunti, as Duryodhana is related to Gandhari\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Run the code cell below to load necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#future is the missing compatibility layer between Python 2 and Python 3. \n",
    "#It allows you to use a single, clean Python 3.x-compatible codebase to \n",
    "#support both Python 2 and Python 3 with minimal overhead.\n",
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#encoding. word encodig\n",
    "import codecs\n",
    "#finds all pathnames matching a pattern, like regex\n",
    "import glob\n",
    "#log events for libraries\n",
    "import logging\n",
    "#concurrency\n",
    "import multiprocessing\n",
    "#dealing with operating system , like reading file\n",
    "import os\n",
    "#pretty print, human readable\n",
    "import pprint\n",
    "#regular expressions\n",
    "import re\n",
    "#natural language toolkit\n",
    "import nltk\n",
    "#word 2 vec\n",
    "import gensim.models.word2vec as w2v\n",
    "#dimensionality reduction\n",
    "import sklearn.manifold\n",
    "#math\n",
    "import numpy as np\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "#parse dataset\n",
    "import pandas as pd\n",
    "#visualization\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download NLTK tokenizer models (only the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DTILAK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DTILAK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##stopwords like the at a an, unnecesasry\n",
    "##tokenization into sentences, punkt \n",
    "##http://www.nltk.org/\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Corpus\n",
    "Load books from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found books:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\1.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\10.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\11.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\12.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\13.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\14.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\15.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\16.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\17.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\18.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\2.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\3.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\4.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\5.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\6.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\7.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\8.txt',\n",
       " '..\\\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\\\input\\\\9.txt']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the book names, matching txt file\n",
    "book_filenames = sorted(glob.glob(\"..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\*.txt\"))\n",
    "print(\"Found books:\")\n",
    "book_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the books into one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\1.txt'...\n",
      "Corpus is now 295412 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\10.txt'...\n",
      "Corpus is now 325640 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\11.txt'...\n",
      "Corpus is now 337972 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\12.txt'...\n",
      "Corpus is now 364937 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\13.txt'...\n",
      "Corpus is now 390015 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\14.txt'...\n",
      "Corpus is now 404662 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\15.txt'...\n",
      "Corpus is now 417222 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\16.txt'...\n",
      "Corpus is now 447521 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\17.txt'...\n",
      "Corpus is now 476028 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\18.txt'...\n",
      "Corpus is now 478927 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\2.txt'...\n",
      "Corpus is now 682505 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\3.txt'...\n",
      "Corpus is now 828766 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\4.txt'...\n",
      "Corpus is now 936868 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\5.txt'...\n",
      "Corpus is now 1052238 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\6.txt'...\n",
      "Corpus is now 1349941 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\7.txt'...\n",
      "Corpus is now 1594913 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\8.txt'...\n",
      "Corpus is now 1664898 characters long\n",
      "\n",
      "Reading '..\\Mahabharata_extract-semantic-similarities_Natural-languageprocessing\\input\\9.txt'...\n",
      "Corpus is now 1706632 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#step 1 process data\n",
    "\n",
    "#initialize raw unicode , we'll add all text to this file in memory\n",
    "corpus_raw = u\"\"\n",
    "\n",
    "#for each book, read it, open it un utf 8 format, \n",
    "#add it to the raw corpus\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print (\"Corpus is now {0} characters long\".format(len(corpus_raw)))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the corpus into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenizastion! saved the trained model here\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize into sentences\n",
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert into a list of words\n",
    "#remove unnnecessary, split into words, no hyphens\n",
    "#list of words\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub(\"[^a-zA-Z]\",\" \", raw)\n",
    "    words = clean.split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentence where each word is tokenized\n",
    "sentences = []\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Above all these qualities, he was a devoted servant of Lord Vishnu, and therefore he was given the title, \"King of kings\".\n",
      "[u'Above', u'all', u'these', u'qualities', u'he', u'was', u'a', u'devoted', u'servant', u'of', u'Lord', u'Vishnu', u'and', u'therefore', u'he', u'was', u'given', u'the', u'title', u'King', u'of', u'kings']\n"
     ]
    }
   ],
   "source": [
    "print(raw_sentences[5])\n",
    "print(sentence_to_wordlist(raw_sentences[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book corpus contains 293,755 tokens\n"
     ]
    }
   ],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ONCE we have vectors\n",
    "#step 3 - build model\n",
    "#3 main tasks that vectors help with\n",
    "#DISTANCE, SIMILARITY, RANKING\n",
    "\n",
    "# Dimensionality of the resulting word vectors.\n",
    "#more dimensions, more computationally expensive to train\n",
    "#but also more accurate\n",
    "#more dimensions = more generalized\n",
    "num_features = 300\n",
    "# Minimum word count threshold.\n",
    "min_word_count = 3\n",
    "\n",
    "# Number of threads to run in parallel.\n",
    "#more workers, faster we train\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# Context window length.\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words.\n",
    "#0 - 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for the RNG, to make the results reproducible.\n",
    "#random number generator\n",
    "#deterministic, good for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mahabharata2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 13:07:41,523 : INFO : collecting all words and their counts\n",
      "2017-03-20 13:07:41,523 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-03-20 13:07:41,569 : INFO : PROGRESS: at sentence #10000, processed 165878 words, keeping 9238 word types\n",
      "2017-03-20 13:07:41,605 : INFO : collected 11439 word types from a corpus of 293755 raw words and 17725 sentences\n",
      "2017-03-20 13:07:41,605 : INFO : Loading a fresh vocabulary\n",
      "2017-03-20 13:07:41,628 : INFO : min_count=3 retains 5703 unique words (49% of original 11439, drops 5736)\n",
      "2017-03-20 13:07:41,631 : INFO : min_count=3 leaves 286413 word corpus (97% of original 293755, drops 7342)\n",
      "2017-03-20 13:07:41,650 : INFO : deleting the raw counts dictionary of 11439 items\n",
      "2017-03-20 13:07:41,651 : INFO : sample=0.001 downsamples 52 most-common words\n",
      "2017-03-20 13:07:41,651 : INFO : downsampling leaves estimated 214323 word corpus (74.8% of prior 286413)\n",
      "2017-03-20 13:07:41,654 : INFO : estimated required memory for 5703 words and 300 dimensions: 16538700 bytes\n",
      "2017-03-20 13:07:41,677 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "mahabharata2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec vocabulary length: 5703\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec vocabulary length:\", len(mahabharata2vec.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training, this might take a minute or two..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 13:07:44,782 : INFO : training model with 4 workers on 5703 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=7\n",
      "2017-03-20 13:07:44,786 : INFO : expecting 17725 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-20 13:07:45,813 : INFO : PROGRESS: at 14.64% examples, 156199 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-20 13:07:46,835 : INFO : PROGRESS: at 31.27% examples, 163907 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-20 13:07:47,842 : INFO : PROGRESS: at 47.59% examples, 167107 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-20 13:07:48,855 : INFO : PROGRESS: at 61.88% examples, 163409 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-20 13:07:49,871 : INFO : PROGRESS: at 78.11% examples, 164859 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-20 13:07:50,948 : INFO : PROGRESS: at 94.27% examples, 164610 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-20 13:07:51,160 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-03-20 13:07:51,191 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-03-20 13:07:51,220 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-03-20 13:07:51,283 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-20 13:07:51,285 : INFO : training on 1468775 raw words (1071197 effective words) took 6.5s, 165000 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1071197"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mahabharata2vec.train(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to file, can be useful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 13:07:54,262 : INFO : saving Word2Vec object under trained\\mahabharata2vec.w2v, separately None\n",
      "2017-03-20 13:07:54,263 : INFO : not storing attribute syn0norm\n",
      "2017-03-20 13:07:54,265 : INFO : not storing attribute cum_table\n",
      "2017-03-20 13:07:54,351 : INFO : saved trained\\mahabharata2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "mahabharata2vec.save(os.path.join(\"trained\", \"mahabharata2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-20 13:07:57,621 : INFO : loading Word2Vec object from trained\\mahabharata2vec.w2v\n",
      "2017-03-20 13:07:57,657 : INFO : loading wv recursively from trained\\mahabharata2vec.w2v.wv.* with mmap=None\n",
      "2017-03-20 13:07:57,660 : INFO : setting ignored attribute syn0norm to None\n",
      "2017-03-20 13:07:57,661 : INFO : setting ignored attribute cum_table to None\n",
      "2017-03-20 13:07:57,665 : INFO : loaded trained\\mahabharata2vec.w2v\n"
     ]
    }
   ],
   "source": [
    "mahabharata2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"mahabharata2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress the word vectors into 2D space and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#my video - how to visualize a dataset easily\n",
    "tsne = sklearn.manifold.TSNE(n_components=3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_word_vectors_matrix = mahabharata2vec.wv.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train t-SNE, this could take a minute or two..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-c9bac182acc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_word_vectors_matrix_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_word_vectors_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\DTILAK\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\manifold\\t_sne.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \"\"\"\n\u001b[0;32m--> 866\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\DTILAK\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\manifold\\t_sne.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"euclidean\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 distances = pairwise_distances(X, metric=self.metric,\n\u001b[0;32m--> 718\u001b[0;31m                                                squared=True)\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\DTILAK\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\pairwise.pyc\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1205\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\DTILAK\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\pairwise.pyc\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[1;31m# Special case to avoid picklability checks in delayed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[1;31m# TODO: in some cases, backend='threading' may be appropriate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\DTILAK\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\metrics\\pairwise.pyc\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mYY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\DTILAK\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\extmath.pyc\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_word_vectors_matrix_2d = tsne.fit_transform(all_word_vectors_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the big picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points = pd.DataFrame(\n",
    "    [\n",
    "        (word, coords[0], coords[1], coords[2])\n",
    "        for word, coords in [\n",
    "            (word, all_word_vectors_matrix_2d[mahabharata2vec.wv.vocab[word].index])\n",
    "            for word in mahabharata2vec.wv.vocab\n",
    "        ]\n",
    "    ],\n",
    "    columns=[\"word\", \"x\", \"y\", \"z\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points.plot.scatter(\"x\", \"y\", c = \"z\",s=10, figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_region(x_bounds, y_bounds):\n",
    "    slice = points[\n",
    "        (x_bounds[0] <= points.x) &\n",
    "        (points.x <= x_bounds[1]) & \n",
    "        (y_bounds[0] <= points.y) &\n",
    "        (points.y <= y_bounds[1])\n",
    "    ]\n",
    "    \n",
    "    ax = slice.plot.scatter(\"x\", \"y\", s=35, figsize=(10, 8))\n",
    "    for i, point in slice.iterrows():\n",
    "        ax.text(point.x + 0.005, point.y + 0.005, point.word, fontsize=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People related to Kingsguard ended up together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_region(x_bounds=(4.0, 4.2), y_bounds=(-0.5, -0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food products are grouped nicely as well. Aerys (The Mad King) being close to \"roasted\" also looks sadly correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_region(x_bounds=(0, 1), y_bounds=(4, 4.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore semantic similarities between book characters. Words closest to the given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mahabharata2vec.most_similar(\"Krishna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mahabharata2vec.most_similar(\"Arjuna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mahabharata2vec.most_similar(\"Karna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mahabharata2vec.most_similar(\"Vrishasena\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear relationships between word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_similarity_cosmul(start1, end1, end2):\n",
    "    similarities = mahabharata2vec.most_similar_cosmul(\n",
    "        positive=[end2, start1],\n",
    "        negative=[end1]\n",
    "    )\n",
    "    start2 = similarities[0][0]\n",
    "    print(\"{start1} is related to {end1}, as {start2} is related to {end2}\".format(**locals()))\n",
    "    return start2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearest_similarity_cosmul(\"Dhritarastra\", \"Pandu\", \"Nakula\")\n",
    "nearest_similarity_cosmul(\"Bhima\", \"Arjuna\", \"Ambika\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "sentence = \"Vrishasena Ambalika at the death of Duhshasana and Chitrasena rushed against Nakula desiring to fight with his father's enemy. A fierce battle then ensued between those two heroes. Vrishasena managed to kill Nakula's horses and pierce him with many arrows. Descending from his chariot, Nakula took up his sword and shield, and making his way toward Vrishasena, he severed the heads of two thousand horsemen. Vrishasena, seeing Nakula coming towards him whirling that sword like a discus, shattered the sword and shield with four crescent shaped arrows. Nakula then quickly ascended Bhima's chariot. As Arjuna came near, Nakula requested him Please slay this sinful person Arjuna then ordered Lord Krishna Proceed toward the son of Karna.\"\n",
    "tagged_sent = pos_tag(sentence.split())\n",
    "print (tagged_sent)\n",
    "\n",
    "propernouns = [word for word,pos in tagged_sent if pos == 'NNP']\n",
    "print (propernouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
